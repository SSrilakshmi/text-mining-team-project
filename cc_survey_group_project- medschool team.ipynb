{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shree\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "import spacy\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "from textblob import Word\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define files\n",
    "filepath = os.getcwd()\n",
    "filename1 = filepath + '\\RawSurveyData2019.csv'\n",
    "filename2 = filepath + '\\SurveyData2016-2018.csv'\n",
    "#print(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv's as a dataframe using pandas\n",
    "newdata = pd.read_csv(filename1)\n",
    "#olddata = pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view data\n",
    "#newdata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view data\n",
    "#olddata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stop words and columns for newdata\n",
    "newdata = pd.DataFrame(newdata)\n",
    "\n",
    "#rename columns\n",
    "newdata.columns = [\"date_opened\",\"category\",\"cc_rating\",\"resolved\", \"contact_customer\", \"cc_comments\",\"eva_rating\", \"eva_comments\", \"cc_specialist\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_calc(text):\n",
    "    try:\n",
    "       # print(text)\n",
    "        return round(TextBlob(text).sentiment.polarity, 2)\n",
    "    except:\n",
    "        return None   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjectivity_calc(text):\n",
    "    try:\n",
    "       # print(text)\n",
    "        return round(TextBlob(text).sentiment.subjectivity, 2)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    try:\n",
    "        # Tokenize into words\n",
    "        tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    \n",
    "        # Remove stopwords\n",
    "        stop = stopwords.words('english')\n",
    "        tokens = [token for token in tokens if token not in stop]\n",
    "        \n",
    "        # Remove punctuation\n",
    "        punc = set(string.punctuation)\n",
    "        tokens = [token for token in tokens if token not in punc]\n",
    "        \n",
    "        # lower case\n",
    "        tokens = [word.lower() for word in tokens]\n",
    "        \n",
    "        # lemmatize\n",
    "        #lmtzr = WordNetLemmatizer()\n",
    "        #tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "        preprocessed_text= ' '.join(tokens)\n",
    "    \n",
    "        return preprocessed_text \n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace [No Answer Entered] with NaN\n",
    "newdata = newdata.replace({'[No Answer Entered]': np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tokenization, stopword, remove punctuation and lower case conversion for cc_comments \n",
    "for column in newdata[['cc_comments']]:\n",
    "    columnValue = newdata[column]\n",
    "    newdata[column+'_polarity_score'] = newdata[column].apply(sentiment_calc)\n",
    "    newdata[column+'_subjectivity_score'] = newdata[column].apply(subjectivity_calc)\n",
    "    newdata[column+'_processed'] = newdata[column].apply(preprocessing)\n",
    "\n",
    "# Perform tokenization, stopword, remove punctuation and Lower case conversion for eva_comments \n",
    "for column in newdata[['eva_comments']]:\n",
    "    newdata[column+'_polarity_score'] = newdata[column].apply(sentiment_calc)\n",
    "    newdata[column+'_subjectivity_score'] = newdata[column].apply(subjectivity_calc)\n",
    "    newdata[column+'_processed'] = newdata[column].apply(preprocessing)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on above code: we can visit stemming if needed because I found this was an issue with the other dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign new columns with interpretations of the sentiment and polarity scores. This was helpful to make charts in Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on cc_comments assign positive, negative, or neutral to polarity in a new column\n",
    "newdata.loc[newdata.cc_comments_polarity_score < 0, 'cc_sentiment'] = 'negative' \n",
    "newdata.loc[newdata.cc_comments_polarity_score == 0, 'cc_sentiment'] = 'neutral' \n",
    "newdata.loc[newdata.cc_comments_polarity_score > 0, 'cc_sentiment'] = 'positive'\n",
    "\n",
    "#on eva_comments assign positive, negative, or neutral to polarity in a new column. \n",
    "newdata.loc[newdata.eva_comments_polarity_score < 0, 'eva_sentiment'] = 'negative' \n",
    "newdata.loc[newdata.eva_comments_polarity_score == 0, 'eva_sentiment'] = 'neutral' \n",
    "newdata.loc[newdata.eva_comments_polarity_score > 0, 'eva_sentiment'] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on cc_comments assign objective or subjective to subjectivity in a new column based on midpoint between 0-1.\n",
    "newdata.loc[newdata.cc_comments_subjectivity_score <= 0.5, 'cc_subjectivity'] = 'objective' \n",
    "newdata.loc[newdata.cc_comments_subjectivity_score > 0.5, 'cc_subjectivity'] = 'subjective'\n",
    "\n",
    "#on eva_comments assign objective or subjective to subjectivity in a new column based on midpoint between 0-1.\n",
    "newdata.loc[newdata.eva_comments_subjectivity_score <= 0.5, 'eva_subjectivity'] = 'objective' \n",
    "newdata.loc[newdata.eva_comments_subjectivity_score > 0.5, 'eva_subjectivity'] = 'subjective'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need to subset each question to remove NaN's? It would make a cleaner set to analyze. I've done that to create a dataset for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_opened</th>\n",
       "      <th>category</th>\n",
       "      <th>cc_rating</th>\n",
       "      <th>resolved</th>\n",
       "      <th>contact_customer</th>\n",
       "      <th>cc_comments</th>\n",
       "      <th>eva_rating</th>\n",
       "      <th>eva_comments</th>\n",
       "      <th>cc_specialist</th>\n",
       "      <th>cc_comments_polarity_score</th>\n",
       "      <th>cc_comments_subjectivity_score</th>\n",
       "      <th>cc_comments_processed</th>\n",
       "      <th>eva_comments_polarity_score</th>\n",
       "      <th>eva_comments_subjectivity_score</th>\n",
       "      <th>eva_comments_processed</th>\n",
       "      <th>cc_sentiment</th>\n",
       "      <th>eva_sentiment</th>\n",
       "      <th>cc_subjectivity</th>\n",
       "      <th>eva_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/24/2019</td>\n",
       "      <td>Vendor</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>It was very quick and easy.</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jennifer, Specialist</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>it quick easy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>subjective</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/24/2019</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>The customer care rep. was very helpful and ve...</td>\n",
       "      <td>5</td>\n",
       "      <td>very satisfied with my experience with the eVA...</td>\n",
       "      <td>Devyonne, Specialist</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>the customer care rep. helpful patient requests</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>satisfied experience eva rep</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>objective</td>\n",
       "      <td>subjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/24/2019</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>We had EP and VDC said it was in composing and...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Portia, Specialist</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>we ep vdc said composing received i told gone ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>objective</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/24/2019</td>\n",
       "      <td>Vendor</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miranda, Specialist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/24/2019</td>\n",
       "      <td>Vendor</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Jennifer was very helpful and resolved my ques...</td>\n",
       "      <td>5</td>\n",
       "      <td>I just contacted the customer care phone number</td>\n",
       "      <td>Jennifer, Specialist</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>jennifer helpful resolved question</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>i contacted customer care phone number</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>objective</td>\n",
       "      <td>objective</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  date_opened category  cc_rating resolved contact_customer  \\\n",
       "0   1/24/2019   Vendor          5      Yes               No   \n",
       "1   1/24/2019    Buyer          5      Yes               No   \n",
       "2   1/24/2019    Buyer          5       No               No   \n",
       "3   1/24/2019   Vendor          5      Yes               No   \n",
       "4   1/24/2019   Vendor          5      Yes               No   \n",
       "\n",
       "                                         cc_comments  eva_rating  \\\n",
       "0                        It was very quick and easy.           5   \n",
       "1  The customer care rep. was very helpful and ve...           5   \n",
       "2  We had EP and VDC said it was in composing and...           5   \n",
       "3                                                NaN           5   \n",
       "4  Jennifer was very helpful and resolved my ques...           5   \n",
       "\n",
       "                                        eva_comments         cc_specialist  \\\n",
       "0                                                NaN  Jennifer, Specialist   \n",
       "1  very satisfied with my experience with the eVA...  Devyonne, Specialist   \n",
       "2                                                NaN    Portia, Specialist   \n",
       "3                                                NaN   Miranda, Specialist   \n",
       "4    I just contacted the customer care phone number  Jennifer, Specialist   \n",
       "\n",
       "   cc_comments_polarity_score  cc_comments_subjectivity_score  \\\n",
       "0                        0.43                            0.74   \n",
       "1                        0.20                            0.30   \n",
       "2                        0.50                            0.50   \n",
       "3                         NaN                             NaN   \n",
       "4                        0.20                            0.30   \n",
       "\n",
       "                               cc_comments_processed  \\\n",
       "0                                      it quick easy   \n",
       "1    the customer care rep. helpful patient requests   \n",
       "2  we ep vdc said composing received i told gone ...   \n",
       "3                                                NaN   \n",
       "4                 jennifer helpful resolved question   \n",
       "\n",
       "   eva_comments_polarity_score  eva_comments_subjectivity_score  \\\n",
       "0                          NaN                              NaN   \n",
       "1                         0.65                              1.0   \n",
       "2                          NaN                              NaN   \n",
       "3                          NaN                              NaN   \n",
       "4                         0.00                              0.0   \n",
       "\n",
       "                   eva_comments_processed cc_sentiment eva_sentiment  \\\n",
       "0                                     NaN     positive           NaN   \n",
       "1            satisfied experience eva rep     positive      positive   \n",
       "2                                     NaN     positive           NaN   \n",
       "3                                     NaN          NaN           NaN   \n",
       "4  i contacted customer care phone number     positive       neutral   \n",
       "\n",
       "  cc_subjectivity eva_subjectivity  \n",
       "0      subjective              NaN  \n",
       "1       objective       subjective  \n",
       "2       objective              NaN  \n",
       "3             NaN              NaN  \n",
       "4       objective        objective  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cc_comments_sentiment_score'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-400-ffddbfde8ae6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#select columns and coll it \"newdata_cc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnewdata_cc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_opened'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'resolved'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cc_rating'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cc_comments'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cc_comments_polarity_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cc_comments_sentiment_score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cc_sentiment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cc_subjectivity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[1;32m-> 2934\u001b[1;33m                                                    raise_missing=True)\n\u001b[0m\u001b[0;32m   2935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[0;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[0;32m   1353\u001b[0m                           raise_missing}\n\u001b[1;32m-> 1354\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[0;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'loc'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not in index\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnot_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['cc_comments_sentiment_score'] not in index\""
     ]
    }
   ],
   "source": [
    "#select columns and call it \"newdata_cc\"\n",
    "newdata_cc = newdata[['date_opened','category', 'resolved','cc_rating', 'cc_comments', 'cc_comments_polarity_score', 'cc_comments_sentiment_score', 'cc_sentiment', 'cc_subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check subset\n",
    "newdata_cc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na rows for newdata_cc_comments\n",
    "newdata_cc = newdata_cc.dropna()\n",
    "\n",
    "newdata_cc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat subset for second survey question eva_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select columns and call it newdata_eva\n",
    "newdata_eva = newdata[['date_opened','category', 'resolved','eva_rating', 'eva_comments', 'eva_comments_polarity_score', 'eva_comments_sentiment_score', 'eva_sentiment', 'eva_subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check subset\n",
    "newdata_eva.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na rows for newdata_cc_comments\n",
    "newdata_eva = newdata_eva.dropna()\n",
    "\n",
    "newdata_eva.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export both question sets as a csv for elasticsearch\n",
    "newdata_cc.to_csv(r'C://Users//annac//Desktop//DAPT//Text Mining//newdata_cc.csv', index = False)\n",
    "newdata_eva.to_csv(r'C://Users//annac//Desktop//DAPT//Text Mining//newdata_eva.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this indexing in Kibana. This must be entered into Kibana under the \"wrench\" tool. \n",
    "#NOT CODE TO RUN IN PYTHON\n",
    "\n",
    "PUT /q1\n",
    "{\n",
    "   \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"std_english\": { \n",
    "          \"type\":      \"standard\",\n",
    "          \"stopwords\": \"_english_\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"resp_id\":    { \"type\": \"keyword\" },  \n",
    "      \"q1\":  { \"type\": \"text\" ,\"analyzer\": \"standard\", \"fielddata\": true }, \n",
    "      \"q1_polarity_score\":   { \"type\": \"integer\"  },     \n",
    "      \"q1_subjectivity_score\":   { \"type\": \"integer\"  },\n",
    "      \"sentiment\":    { \"type\": \"keyword\" },\n",
    "      \"subjectivity\":    { \"type\": \"keyword\" }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Once you enter this in Kibana, THEN you can run the below code to push the data in and it will index properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code used to push to elasticsearch (after index mapping above is set in Kibana)\n",
    "from elasticsearch import helpers, Elasticsearch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q1.csv') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    helpers.bulk(es, reader, index='q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can repeat this code if we want to for the \"olddata\" file we imported which is from 2016-2018. NOTE: this dataset only has 1 survey question on Customer Care comments (no eva comments- that was added in 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we finalize the code and all of us get the data into kibana, we can make some visualizations and screen shot them. I can show you that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
